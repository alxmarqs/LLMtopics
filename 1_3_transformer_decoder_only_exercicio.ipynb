{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alxmarqs/LLMtopics/blob/main/1_3_transformer_decoder_only_exercicio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2uGZJm_9--U_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdcH-tn8-957"
      },
      "source": [
        "# Aula 3 — Exercício: Transformer Decoder-only com Inferência\n",
        "\n",
        "## Características do Decoder-only\n",
        "- **Máscara causal**: cada token só enxerga tokens anteriores (nunca o futuro)\n",
        "- **Auto-regressão**: gera um token por vez, realimentando a saída na entrada\n",
        "- Sem cross-attention, sem encoder separado\n",
        "\n",
        "## O que este exercício implementa\n",
        "1. Carregamento do corpus (IMDB-Genres)\n",
        "2. Treinamento de tokenizador WordLevel\n",
        "3. Treinamento do modelo decoder-only (linguagem causal)\n",
        "4. **Inferência com máxima probabilidade** (greedy) no loop de treino\n",
        "5. **Inferência com amostragem por temperatura** (+ top-k / top-p) no loop de treino\n",
        "6. Visualização interativa dos efeitos de temperatura, top-k e top-p **com logits reais do modelo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weMq8Lq2-959"
      },
      "source": [
        "## Instalação de dependências"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPV6fqo3-959"
      },
      "outputs": [],
      "source": [
        "!pip install datasets tokenizers -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrZ7wZcr-95-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import random\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Dispositivo: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiCRa151-95-"
      },
      "source": [
        "---\n",
        "## Passo 1 — Carregar o conjunto de documentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Awn0JvL3-95_"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"jquigl/imdb-genres\")\n",
        "\n",
        "def clean_ascii(text):\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    return re.sub(r\"[^A-Za-z0-9 .,:;!?'\\-]\", \"\", text)\n",
        "\n",
        "documentos = [clean_ascii(x[\"description\"]) for x in ds[\"train\"]]\n",
        "documentos = [t.split(\" - \")[0] for t in documentos]\n",
        "documentos = [t for t in documentos if len(t) > 20]\n",
        "\n",
        "print(f\"Total de documentos: {len(documentos)}\")\n",
        "print(\"\\nAmostras:\")\n",
        "for d in documentos[:5]:\n",
        "    print(\" •\", d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIdyJkHr-95_"
      },
      "source": [
        "---\n",
        "## Passo 2 — Treinar o tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zboUy6b-95_"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer = WordLevelTrainer(\n",
        "    vocab_size=2000,\n",
        "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"]\n",
        ")\n",
        "\n",
        "tokenizer.train_from_iterator(documentos, trainer)\n",
        "\n",
        "vocab_size = tokenizer.get_vocab_size()\n",
        "pad_token_id = tokenizer.token_to_id(\"[PAD]\")\n",
        "bos_token_id = tokenizer.token_to_id(\"[BOS]\")\n",
        "eos_token_id = tokenizer.token_to_id(\"[EOS]\")\n",
        "\n",
        "print(f\"Vocabulário: {vocab_size} tokens\")\n",
        "print(f\"Especiais — PAD:{pad_token_id} | BOS:{bos_token_id} | EOS:{eos_token_id}\")\n",
        "\n",
        "def encode(text):\n",
        "    ids = tokenizer.encode(\"[BOS] \" + text + \" [EOS]\").ids\n",
        "    return torch.tensor(ids, dtype=torch.long)\n",
        "\n",
        "def decode(ids):\n",
        "    \"\"\"Decodifica IDs filtrando tokens especiais.\"\"\"\n",
        "    special = {pad_token_id, bos_token_id, eos_token_id}\n",
        "    clean = [i for i in (ids.tolist() if hasattr(ids, 'tolist') else ids) if i not in special]\n",
        "    return tokenizer.decode(clean)\n",
        "\n",
        "# Exemplo\n",
        "ex = encode(documentos[0])\n",
        "print(f\"\\nExemplo: '{documentos[0][:50]}...'\")\n",
        "print(f\"Tokens ({len(ex)}): {ex.tolist()[:10]} ...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ooDPpjN-95_"
      },
      "source": [
        "---\n",
        "## Passo 3 — Definição do modelo Transformer Decoder-only\n",
        "\n",
        "A **máscara causal** é o que diferencia o decoder do encoder:\n",
        "- Cada posição `t` só consegue \"ver\" as posições `0..t` (contexto passado)\n",
        "- Isso é essencial para gerar texto de forma autoregressiva: o modelo nunca \"espiona\" o futuro durante o treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEvUWkBp-96A"
      },
      "outputs": [],
      "source": [
        "class DecoderOnlyTransformer(nn.Module):\n",
        "    \"\"\"Transformer Decoder-only para geração de texto (linguagem causal).\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, d_model=128, n_heads=4, num_layers=3, max_len=64):\n",
        "        super().__init__()\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
        "\n",
        "        # TransformerDecoder usado sem memória de encoder (self-attention causal)\n",
        "        layer = nn.TransformerDecoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=256,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(layer, num_layers=num_layers)\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def _causal_mask(self, T, device):\n",
        "        \"\"\"Máscara triangular superior: impede ver tokens futuros.\"\"\"\n",
        "        return torch.triu(torch.ones(T, T, device=device), diagonal=1).bool()\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T = x.shape\n",
        "        pos = torch.arange(T, device=x.device).unsqueeze(0)\n",
        "        h = self.token_emb(x) + self.pos_emb(pos)  # (B, T, d_model)\n",
        "\n",
        "        mask = self._causal_mask(T, x.device)\n",
        "        out = self.decoder(h, h, tgt_mask=mask)  # h como memória (sem encoder)\n",
        "        logits = self.lm_head(out)  # (B, T, vocab_size)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TotKkBGG-96A"
      },
      "source": [
        "---\n",
        "## Passo 4 — Estratégias de inferência\n",
        "\n",
        "### 4a. Máxima probabilidade (greedy)\n",
        "Sempre escolhe o token com maior probabilidade. É determinístico, rápido, mas pode ficar repetitivo.\n",
        "\n",
        "### 4b. Amostragem com temperatura + Top-K + Top-P (nucleus sampling)\n",
        "- **Temperatura** controla a \"criatividade\": valores baixos → mais conservador; valores altos → mais aleatório\n",
        "- **Top-K** filtra os K tokens mais prováveis antes de amostrar\n",
        "- **Top-P (nucleus)** filtra os tokens que juntos somam probabilidade ≥ P"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GL70kXSM-96A"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------\n",
        "# 4a. Greedy: token com maior probabilidade\n",
        "# -------------------------------------------------------\n",
        "def max_prob_sampling(logits):\n",
        "    \"\"\"\n",
        "    Seleção greedy: retorna o índice do token com maior logit.\n",
        "    logits: tensor (vocab_size,)\n",
        "    retorna: tensor (1,)\n",
        "    \"\"\"\n",
        "    return logits.argmax(dim=-1, keepdim=True)  # (1,)\n",
        "\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 4b. Amostragem com temperatura, top-k e top-p\n",
        "# -------------------------------------------------------\n",
        "def sampling(logits, temperature=1.0, top_k=None, top_p=0.9):\n",
        "    \"\"\"\n",
        "    Amostragem estocástica com:\n",
        "    - temperature: divide os logits (> 1 = mais aleatório, < 1 = mais concentrado)\n",
        "    - top_k: mantém apenas os K tokens mais prováveis\n",
        "    - top_p (nucleus): mantém o menor conjunto com prob acumulada >= p\n",
        "    logits: tensor (vocab_size,)\n",
        "    retorna: tensor (1,)\n",
        "    \"\"\"\n",
        "    logits = logits.clone().float()\n",
        "\n",
        "    # --- Temperatura ---\n",
        "    logits = logits / max(temperature, 1e-5)\n",
        "\n",
        "    # --- Top-K ---\n",
        "    if top_k is not None:\n",
        "        k = min(top_k, logits.size(-1))\n",
        "        top_vals, _ = torch.topk(logits, k)\n",
        "        min_val = top_vals[-1]  # menor valor entre os top-k\n",
        "        logits = logits.masked_fill(logits < min_val, float('-inf'))\n",
        "\n",
        "    # --- Top-P (nucleus) ---\n",
        "    sorted_logits, sorted_idx = torch.sort(logits, descending=True)\n",
        "    cum_probs = torch.softmax(sorted_logits, dim=-1).cumsum(dim=-1)\n",
        "\n",
        "    # Remove tokens cuja prob acumulada ultrapassa p (exceto o primeiro)\n",
        "    remove_mask = cum_probs > top_p\n",
        "    remove_mask[..., 1:] = remove_mask[..., :-1].clone()\n",
        "    remove_mask[..., 0] = False\n",
        "\n",
        "    filtered = sorted_logits.masked_fill(remove_mask, float('-inf'))\n",
        "    probs = torch.softmax(filtered, dim=-1)\n",
        "\n",
        "    # Amostra e mapeia de volta ao índice original\n",
        "    sampled_rank = torch.multinomial(probs, num_samples=1)  # posição no sorted\n",
        "    next_token = sorted_idx[sampled_rank]  # índice real no vocab\n",
        "\n",
        "    return next_token  # (1,)\n",
        "\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Função de geração autoregressiva (comum às duas estratégias)\n",
        "# -------------------------------------------------------\n",
        "def generate(prompt, strategy='greedy', max_new_tokens=20,\n",
        "             temperature=1.0, top_k=None, top_p=0.9):\n",
        "    \"\"\"\n",
        "    Gera texto a partir de um prompt usando o modelo decoder-only.\n",
        "\n",
        "    strategy:\n",
        "        'greedy' → max_prob_sampling (determinístico)\n",
        "        'sampling' → amostragem com temperatura/top-k/top-p\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x = encode(prompt).unsqueeze(0).to(device)  # (1, T)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Trunca se exceder max_len do modelo\n",
        "            x_cond = x[:, -model.max_len:]\n",
        "\n",
        "            logits_all = model(x_cond)  # (1, T, vocab_size)\n",
        "            logits = logits_all[0, -1]  # último passo: (vocab_size,)\n",
        "\n",
        "            if strategy == 'greedy':\n",
        "                next_token = max_prob_sampling(logits)  # (1,)\n",
        "            else:  # 'sampling'\n",
        "                next_token = sampling(logits,\n",
        "                                    temperature=temperature,\n",
        "                                    top_k=top_k,\n",
        "                                    top_p=top_p)  # (1,)\n",
        "\n",
        "            x = torch.cat([x, next_token.unsqueeze(0)], dim=1)  # (1, T+1)\n",
        "\n",
        "            if next_token.item() == eos_token_id:\n",
        "                break\n",
        "\n",
        "        return decode(x[0])\n",
        "\n",
        "print(\"Funções de inferência definidas.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiOnEuDo-96A"
      },
      "source": [
        "---\n",
        "## Passo 5 — Treinamento do modelo com inferência periódica\n",
        "\n",
        "A cada 500 steps, o loop **interrompe** o treino e gera texto com as duas estratégias:\n",
        "- **Greedy** — saída determinística e conservadora\n",
        "- **Sampling** — saída estocástica e mais variada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7GUqbQ3-96A"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 32\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def sample_batch(batch_size=BATCH_SIZE, max_len=MAX_LEN):\n",
        "    \"\"\"Amostra um batch, tokeniza e faz padding com [PAD].\"\"\"\n",
        "    batch = random.sample(documentos, batch_size)\n",
        "    tokenized = [encode(t) for t in batch]\n",
        "    max_t = min(max(len(x) for x in tokenized), max_len)\n",
        "    padded = []\n",
        "    for x in tokenized:\n",
        "        x = x[:max_t]\n",
        "        pad_len = max_t - len(x)\n",
        "        if pad_len > 0:\n",
        "            x = torch.cat([x, torch.full((pad_len,), pad_token_id, dtype=torch.long)])\n",
        "        padded.append(x)\n",
        "    return torch.stack(padded)  # (B, T)\n",
        "\n",
        "\n",
        "# ---- instancia modelo e otimizador ----\n",
        "model = DecoderOnlyTransformer(vocab_size).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Parâmetros totais: {total_params:,}\")\n",
        "print(f\"Dispositivo: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xy-m9ucY-96A"
      },
      "outputs": [],
      "source": [
        "STEPS = 5000\n",
        "LOG_EVERY = 200  # imprime loss\n",
        "GEN_EVERY = 500  # gera texto com as duas estratégias\n",
        "PROMPT = \"This movie is a\"\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "print(f\"Treinando por {STEPS} steps...\")\n",
        "print(f\"Prompt de teste: '{PROMPT}'\\n\")\n",
        "print(\"=\" * 65)\n",
        "\n",
        "for step in range(1, STEPS + 1):\n",
        "    model.train()\n",
        "    batch = sample_batch().to(device)\n",
        "\n",
        "    # Objetivo causal: prever batch[:, 1:] dado batch[:, :-1]\n",
        "    logits = model(batch[:, :-1])\n",
        "    loss = F.cross_entropy(\n",
        "        logits.reshape(-1, vocab_size),\n",
        "        batch[:, 1:].reshape(-1),\n",
        "        ignore_index=pad_token_id\n",
        "    )\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_history.append(loss.item())\n",
        "\n",
        "    # --- Log periódico ---\n",
        "    if step % LOG_EVERY == 0:\n",
        "        avg_loss = np.mean(loss_history[-LOG_EVERY:])\n",
        "        ppl = np.exp(avg_loss)\n",
        "        print(f\"[step {step:>5}] loss = {avg_loss:.4f} | perplexidade = {ppl:.2f}\")\n",
        "\n",
        "    # --- Geração com as duas estratégias ---\n",
        "    if step % GEN_EVERY == 0:\n",
        "        print(f\"\\n{'─'*65}\")\n",
        "        print(f\"  Geração no step {step}\")\n",
        "        print(f\"  Prompt: '{PROMPT}'\")\n",
        "        print(f\"{'─'*65}\")\n",
        "\n",
        "        # Estratégia 1: greedy (máxima probabilidade)\n",
        "        texto_greedy = generate(\n",
        "            PROMPT,\n",
        "            strategy='greedy',\n",
        "            max_new_tokens=15\n",
        "        )\n",
        "        print(f\"  [Greedy] {texto_greedy}\")\n",
        "\n",
        "        # Estratégia 2: amostragem com temperatura\n",
        "        texto_sample = generate(\n",
        "            PROMPT,\n",
        "            strategy='sampling',\n",
        "            max_new_tokens=15,\n",
        "            temperature=0.8,\n",
        "            top_k=50,\n",
        "            top_p=0.9\n",
        "        )\n",
        "        print(f\"  [Sampling T=0.8, K=50, P=0.9] {texto_sample}\")\n",
        "        print()\n",
        "\n",
        "print(\"=\" * 65)\n",
        "print(\"Treinamento concluído!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vcw3bsu4-96A"
      },
      "outputs": [],
      "source": [
        "# Curva de loss com média móvel\n",
        "window = 100\n",
        "smooth = np.convolve(loss_history, np.ones(window)/window, mode='valid')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 3))\n",
        "ax.plot(loss_history, alpha=0.2, color='royalblue')\n",
        "ax.plot(range(window-1, len(loss_history)), smooth, color='royalblue', lw=2,\n",
        "        label=f'Média móvel ({window} steps)')\n",
        "ax.set_xlabel('Step')\n",
        "ax.set_ylabel('Cross-Entropy Loss')\n",
        "ax.set_title('Curva de Treinamento — Decoder-only (Linguagem Causal)')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwTnsTZh-96B"
      },
      "source": [
        "---\n",
        "## Passo 6 — Comparação de estratégias de geração\n",
        "\n",
        "Agora comparamos as diferentes configurações sobre o **mesmo prompt**, com o modelo já treinado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snG1sahd-96B"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    \"This movie is a\",\n",
        "    \"A story about love and\",\n",
        "    \"In the future, the world\",\n",
        "]\n",
        "\n",
        "configs = [\n",
        "    {\"label\": \"Greedy\", \"strategy\": \"greedy\"},\n",
        "    {\"label\": \"Temp=0.5 K=10 P=0.9\", \"strategy\": \"sampling\", \"temperature\": 0.5, \"top_k\": 10, \"top_p\": 0.9},\n",
        "    {\"label\": \"Temp=1.0 K=50 P=0.9\", \"strategy\": \"sampling\", \"temperature\": 1.0, \"top_k\": 50, \"top_p\": 0.9},\n",
        "    {\"label\": \"Temp=1.5 K=None P=0.95\", \"strategy\": \"sampling\", \"temperature\": 1.5, \"top_k\": None, \"top_p\": 0.95},\n",
        "]\n",
        "\n",
        "print(\"Comparação de estratégias de geração\\n\")\n",
        "for prompt in prompts:\n",
        "    print(f\"  Prompt: '{prompt}'\")\n",
        "    print(\"  \" + \"-\" * 55)\n",
        "    for cfg in configs:\n",
        "        kw = {k: v for k, v in cfg.items() if k not in ('label', 'strategy')}\n",
        "        texto = generate(prompt, strategy=cfg['strategy'], max_new_tokens=15, **kw)\n",
        "        print(f\"  [{cfg['label']:<25}] {texto}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M0oAWAo-96B"
      },
      "source": [
        "---\n",
        "## Passo 7 — Visualização interativa: Temperatura, Top-K, Top-P\n",
        "\n",
        "Diferente do exemplo original (que usava logits fictícios), aqui usamos **logits reais do modelo treinado** para um prompt real. Isso torna a visualização diretamente conectada ao comportamento aprendido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hL5rd8iX-96B"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Funções NumPy para a visualização (replicam a lógica PyTorch)\n",
        "# ------------------------------------------------------------------\n",
        "def np_softmax(x):\n",
        "    e = np.exp(x - np.max(x))\n",
        "    return e / e.sum()\n",
        "\n",
        "def apply_temperature_np(logits, temperature):\n",
        "    return logits / max(temperature, 1e-5)\n",
        "\n",
        "def apply_top_k_np(logits, k):\n",
        "    if k is None or k >= len(logits):\n",
        "        return logits\n",
        "    threshold = np.sort(logits)[::-1][k - 1]\n",
        "    result = logits.copy()\n",
        "    result[result < threshold] = -np.inf\n",
        "    return result\n",
        "\n",
        "def apply_top_p_np(logits, p):\n",
        "    sorted_idx = np.argsort(logits)[::-1]\n",
        "    sorted_logs = logits[sorted_idx]\n",
        "    cum_probs = np.cumsum(np_softmax(sorted_logs))\n",
        "    remove = cum_probs > p\n",
        "    remove[1:] = remove[:-1].copy()\n",
        "    remove[0] = False\n",
        "    result = logits.copy()\n",
        "    result[sorted_idx[remove]] = -np.inf\n",
        "    return result\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Extrai logits reais para um prompt\n",
        "# ------------------------------------------------------------------\n",
        "def get_real_logits(prompt, top_n=15):\n",
        "    \"\"\"Retorna os top_n tokens e logits reais do modelo para o próximo token.\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x = encode(prompt).unsqueeze(0).to(device)\n",
        "        logits = model(x)[0, -1].cpu().numpy()  # (vocab_size,)\n",
        "\n",
        "    top_idx = np.argsort(logits)[::-1][:top_n]\n",
        "    top_logits = logits[top_idx]\n",
        "    top_tokens = [tokenizer.id_to_token(int(i)) for i in top_idx]\n",
        "    return top_tokens, top_logits, top_idx, logits\n",
        "\n",
        "\n",
        "PROMPT_VIZ = \"This movie is a\"\n",
        "TOKENS_VIZ, BASE_LOGITS_VIZ, TOP_IDX_VIZ, FULL_LOGITS_VIZ = get_real_logits(PROMPT_VIZ)\n",
        "\n",
        "print(f\"Prompt: '{PROMPT_VIZ}'\")\n",
        "print(f\"Top 5 tokens: {TOKENS_VIZ[:5]}\")\n",
        "print(f\"Top 5 logits: {BASE_LOGITS_VIZ[:5].round(2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qY-pE1fB-96B"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------\n",
        "# Widgets interativos\n",
        "# ------------------------------------------------------------------\n",
        "prompt_widget = widgets.Dropdown(\n",
        "    options=[\"This movie is a\", \"A story about love and\", \"In the future, the world\"],\n",
        "    value=\"This movie is a\",\n",
        "    description=\"Prompt:\",\n",
        "    layout=widgets.Layout(width=\"95%\")\n",
        ")\n",
        "\n",
        "temp_slider = widgets.FloatSlider(\n",
        "    value=1.0, min=0.1, max=3.0, step=0.1,\n",
        "    description=\"Temperatura:\",\n",
        "    readout_format=\".1f\",\n",
        "    layout=widgets.Layout(width=\"90%\")\n",
        ")\n",
        "\n",
        "topk_slider = widgets.IntSlider(\n",
        "    value=15, min=1, max=15, step=1,\n",
        "    description=\"Top-K:\",\n",
        "    layout=widgets.Layout(width=\"90%\")\n",
        ")\n",
        "\n",
        "topp_slider = widgets.FloatSlider(\n",
        "    value=1.0, min=0.1, max=1.0, step=0.05,\n",
        "    description=\"Top-P:\",\n",
        "    readout_format=\".2f\",\n",
        "    layout=widgets.Layout(width=\"90%\")\n",
        ")\n",
        "\n",
        "output_plot = widgets.Output()\n",
        "\n",
        "\n",
        "def update_plot(*args):\n",
        "    \"\"\"Atualiza o gráfico com logits reais do modelo.\"\"\"\n",
        "    tokens_viz, base_logits, _, _ = get_real_logits(prompt_widget.value)\n",
        "\n",
        "    # Aplica transformações\n",
        "    logits = apply_temperature_np(base_logits.copy(), temp_slider.value)\n",
        "    logits = apply_top_k_np(logits, topk_slider.value)\n",
        "    logits = apply_top_p_np(logits, topp_slider.value)\n",
        "    probs = np_softmax(logits)\n",
        "\n",
        "    # Cores: azul = disponível, cinza = filtrado (-inf)\n",
        "    colors = ['#2196F3' if np.isfinite(l) else '#BDBDBD' for l in logits]\n",
        "\n",
        "    with output_plot:\n",
        "        output_plot.clear_output(wait=True)\n",
        "        fig, ax = plt.subplots(figsize=(11, 4))\n",
        "        bars = ax.bar(tokens_viz, probs * 100, color=colors, edgecolor='white')\n",
        "\n",
        "        for bar, p, l in zip(bars, probs, logits):\n",
        "            if np.isfinite(l):\n",
        "                ax.text(bar.get_x() + bar.get_width()/2,\n",
        "                        bar.get_height() + 0.5,\n",
        "                        f\"{p*100:.1f}%\",\n",
        "                        ha='center', va='bottom', fontsize=8)\n",
        "            else:\n",
        "                ax.text(bar.get_x() + bar.get_width()/2,\n",
        "                        1, \"X\", ha='center', va='bottom', fontsize=9, color='gray')\n",
        "\n",
        "        ax.set_ylim(0, max(probs)*100 * 1.25 + 2)\n",
        "        ax.set_ylabel(\"Probabilidade (%)\")\n",
        "        ax.set_title(\n",
        "            f\"Próximo token após: '{prompt_widget.value}'\\n\"\n",
        "            f\"[Logits reais do modelo — T={temp_slider.value:.1f} K={topk_slider.value} P={topp_slider.value:.2f}]\",\n",
        "            fontsize=10\n",
        "        )\n",
        "        ax.tick_params(axis='x', rotation=30)\n",
        "        ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "        # Legenda\n",
        "        from matplotlib.patches import Patch\n",
        "        legend_elem = [\n",
        "            Patch(facecolor='#2196F3', label='Token disponível'),\n",
        "            Patch(facecolor='#BDBDBD', label='Filtrado (Top-K/P)'),\n",
        "        ]\n",
        "        ax.legend(handles=legend_elem, fontsize=8, loc='upper right')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "for w in [temp_slider, topk_slider, topp_slider, prompt_widget]:\n",
        "    w.observe(update_plot, names='value')\n",
        "\n",
        "# Exibe o painel\n",
        "panel = widgets.VBox([\n",
        "    widgets.HTML(\"<h3>Visualização Interativa: Temperatura, Top-K e Top-P</h3>\"),\n",
        "    prompt_widget,\n",
        "    temp_slider,\n",
        "    topk_slider,\n",
        "    topp_slider,\n",
        "    output_plot\n",
        "])\n",
        "\n",
        "display(panel)\n",
        "update_plot()  # Gera a primeira visualização"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPJDyMDS-96B"
      },
      "source": [
        "---\n",
        "## Conclusão\n",
        "\n",
        "Neste notebook implementamos:\n",
        "\n",
        "1. Um **Transformer Decoder-only** com máscara causal\n",
        "2. **Inferência greedy** (determinística, máxima probabilidade)\n",
        "3. **Inferência estocástica** com temperatura, top-k e top-p\n",
        "4. Treinamento com geração periódica no loop\n",
        "5. Comparação de múltiplas estratégias de geração\n",
        "6. Visualização interativa com **logits reais do modelo**\n",
        "\n",
        "### Principais Conceitos\n",
        "\n",
        "- **Máscara causal**: Permite que cada posição veja apenas o passado (essencial para modelos autoregressivos)\n",
        "- **Temperatura**: Controla a \"criatividade\" da geração (baixo = conservador, alto = aleatório)\n",
        "- **Top-K**: Limita candidatos aos K tokens mais prováveis\n",
        "- **Top-P (nucleus)**: Limita candidatos ao menor conjunto cuja probabilidade acumulada ≥ P\n",
        "\n",
        "### Próximos Passos\n",
        "\n",
        "- Experimente diferentes configurações de temperatura, top-k e top-p\n",
        "- Teste com prompts personalizados\n",
        "- Aumente o número de steps de treinamento para melhorar a qualidade\n",
        "- Explore diferentes arquiteturas (mais camadas, mais heads de atenção)"
      ]
    }
  ]
}