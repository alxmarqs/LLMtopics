{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alxmarqs/LLMtopics/blob/main/4_exercicio_comparacao_atencao.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crskujphyxP_"
      },
      "source": [
        "# Exercício: Comparação Encoder-Decoder COM e SEM Atenção (Luong)\n",
        "\n",
        "Este notebook compara dois modelos Seq2Seq baseados em GRU para a tarefa de **inverter sequências de caracteres** (ex: `abcd` → `dcba`):\n",
        "\n",
        "- **Modelo COM atenção**: usa Luong Attention (dot product)\n",
        "- **Modelo SEM atenção**: decoder simples sem mecanismo de atenção\n",
        "\n",
        "Ao final, comparamos a perda (loss) ao longo do treinamento e a acurácia por sequência."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWZfS94lyxQD"
      },
      "source": [
        "## 1. Preparação dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MATxncM0yxQD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "chars = list(\"abcd \")\n",
        "vocab = {ch: i for i, ch in enumerate(chars)}\n",
        "inv_vocab = {i: ch for ch, i in vocab.items()}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "def encode(s):\n",
        "    return torch.tensor([vocab[c] for c in s], dtype=torch.long)\n",
        "\n",
        "def decode(t):\n",
        "    return ''.join(inv_vocab[int(x)] for x in t)\n",
        "\n",
        "def random_seq(n=5):\n",
        "    return ''.join(random.choice(chars[:-1]) for _ in range(n))\n",
        "\n",
        "# Gerar dados (mesmos dados para ambos os modelos)\n",
        "seqs = [random_seq() for _ in range(50000)]\n",
        "pairs = [(encode(s), encode(s[::-1])) for s in seqs]\n",
        "\n",
        "max_len = max(len(x) for x, _ in pairs)\n",
        "\n",
        "def pad(x):\n",
        "    return torch.cat([x, torch.tensor([vocab[' ']] * (max_len - len(x)))], dim=0)\n",
        "\n",
        "inputs  = torch.stack([pad(x) for x, _ in pairs])\n",
        "targets = torch.stack([pad(y) for _, y in pairs])\n",
        "\n",
        "train_ds = torch.utils.data.TensorDataset(inputs, targets)\n",
        "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Dispositivo: {device}')\n",
        "print(f'Exemplo de par - entrada: {decode(pairs[0][0])}  |  saída: {decode(pairs[0][1])}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "li6w7_6qyxQE"
      },
      "source": [
        "## 2. Definição dos modelos\n",
        "\n",
        "### 2.1 Encoder (compartilhado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nygScRkyyxQE"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, emb_size)\n",
        "        self.gru   = nn.GRU(emb_size, hidden_size, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        outputs, h = self.gru(x)\n",
        "        return outputs, h   # outputs: (B, S, H)  |  h: (1, B, H)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-GBm_RFyxQF"
      },
      "source": [
        "### 2.2 Modelo COM atenção de Luong (dot-product)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYkPPRR3yxQF"
      },
      "outputs": [],
      "source": [
        "class LuongAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, decoder_hidden, encoder_outputs):\n",
        "        \"\"\"\n",
        "        decoder_hidden  : (B, 1, H)\n",
        "        encoder_outputs : (B, S, H)\n",
        "        \"\"\"\n",
        "        # score = h_t · h_s^T  →  (B, 1, S)\n",
        "        attn_scores  = torch.bmm(decoder_hidden, encoder_outputs.transpose(1, 2))\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "        # context = soma ponderada  →  (B, 1, H)\n",
        "        context = torch.bmm(attn_weights, encoder_outputs)\n",
        "        return context, attn_weights\n",
        "\n",
        "\n",
        "class DecoderComAtencao(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, emb_size)\n",
        "        self.gru   = nn.GRU(emb_size, hidden_size, batch_first=True)\n",
        "        self.attn  = LuongAttention()\n",
        "        # [h_t ; context]  →  vocab\n",
        "        self.fc    = nn.Linear(hidden_size * 2, vocab_size)\n",
        "\n",
        "    def forward(self, x, h, encoder_outputs):\n",
        "        x = self.embed(x)          # (B, T, E)\n",
        "        outputs = []\n",
        "        hidden  = h\n",
        "        for t in range(x.size(1)):\n",
        "            inp = x[:, t:t+1]                              # (B, 1, E)\n",
        "            out_t, hidden = self.gru(inp, hidden)          # (B, 1, H)\n",
        "            context, _    = self.attn(out_t, encoder_outputs)\n",
        "            combined      = torch.cat([out_t, context], dim=-1)\n",
        "            outputs.append(self.fc(combined))\n",
        "        return torch.cat(outputs, dim=1), hidden           # (B, T, V)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UH5CEg2yxQF"
      },
      "source": [
        "### 2.3 Modelo SEM atenção"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0Udc_NpyxQF"
      },
      "outputs": [],
      "source": [
        "class DecoderSemAtencao(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder padrão sem mecanismo de atenção.\n",
        "    Usa apenas o estado oculto do GRU para gerar a saída.\n",
        "    O contexto é simplesmente ignorado — toda a informação da\n",
        "    sequência de entrada precisa estar comprimida no vetor h.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, emb_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, emb_size)\n",
        "        self.gru   = nn.GRU(emb_size, hidden_size, batch_first=True)\n",
        "        # apenas h_t  →  vocab  (sem concatenar contexto)\n",
        "        self.fc    = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, h, encoder_outputs=None):\n",
        "        # encoder_outputs aceito mas ignorado para manter interface igual\n",
        "        x = self.embed(x)          # (B, T, E)\n",
        "        outputs = []\n",
        "        hidden  = h\n",
        "        for t in range(x.size(1)):\n",
        "            inp = x[:, t:t+1]                        # (B, 1, E)\n",
        "            out_t, hidden = self.gru(inp, hidden)    # (B, 1, H)\n",
        "            outputs.append(self.fc(out_t))           # apenas h_t\n",
        "        return torch.cat(outputs, dim=1), hidden     # (B, T, V)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BV8mbLu_yxQG"
      },
      "source": [
        "### 2.4 Módulo Seq2Seq genérico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MC-Ewz2jyxQG"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        encoder_outputs, h = self.encoder(src)\n",
        "        logits, _          = self.decoder(tgt[:, :-1], h, encoder_outputs)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C5Y8olRyxQG"
      },
      "source": [
        "## 3. Funções de inferência"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxiNjwNtyxQG"
      },
      "outputs": [],
      "source": [
        "def predict(model, seq, max_len=10):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src = pad(encode(seq)).unsqueeze(0).to(device, dtype=torch.long)\n",
        "        encoder_outputs, h = model.encoder(src)\n",
        "        token = torch.tensor([[vocab[' ']]], dtype=torch.long, device=device)\n",
        "        result = []\n",
        "        for _ in range(max_len):\n",
        "            logits, h = model.decoder(token, h, encoder_outputs)\n",
        "            token = logits[:, -1, :].argmax(-1, keepdim=True)\n",
        "            result.append(token.item())\n",
        "        return decode(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQcGURqOyxQG"
      },
      "source": [
        "## 4. Treinamento\n",
        "\n",
        "Ambos os modelos usam os mesmos hiperparâmetros e dados para uma comparação justa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNtryR30yxQG"
      },
      "outputs": [],
      "source": [
        "EMB_SIZE    = 32\n",
        "HIDDEN_SIZE = 64\n",
        "EPOCHS      = 10\n",
        "LR          = 1e-3\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=vocab[' '])\n",
        "\n",
        "def build_model(decoder_cls):\n",
        "    enc = Encoder(vocab_size, EMB_SIZE, HIDDEN_SIZE)\n",
        "    dec = decoder_cls(vocab_size, EMB_SIZE, HIDDEN_SIZE)\n",
        "    return Seq2Seq(enc, dec).to(device)\n",
        "\n",
        "def train(model, label):\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "    history = []\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total = 0\n",
        "        for xb, yb in train_dl:\n",
        "            xb, yb = xb.to(device, dtype=torch.long), yb.to(device, dtype=torch.long)\n",
        "            opt.zero_grad()\n",
        "            logits = model(xb, yb)\n",
        "            loss   = loss_fn(logits.reshape(-1, vocab_size), yb[:, 1:].reshape(-1))\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total += loss.item()\n",
        "        avg = total / len(train_dl)\n",
        "        history.append(avg)\n",
        "        print(f\"[{label}] Epoch {epoch+1:2d}: loss = {avg:.4f}\")\n",
        "    return history\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"Treinando modelo COM atenção de Luong...\")\n",
        "print(\"=\" * 50)\n",
        "model_com = build_model(DecoderComAtencao)\n",
        "hist_com  = train(model_com, \"COM atenção\")\n",
        "\n",
        "print()\n",
        "print(\"=\" * 50)\n",
        "print(\"Treinando modelo SEM atenção...\")\n",
        "print(\"=\" * 50)\n",
        "model_sem = build_model(DecoderSemAtencao)\n",
        "hist_sem  = train(model_sem, \"SEM atenção\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1a0ovLnyxQG"
      },
      "source": [
        "## 5. Comparação da Loss durante o treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GsJGqkxyxQG"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(range(1, EPOCHS+1), hist_com, marker='o', label='COM atenção (Luong)', color='steelblue')\n",
        "plt.plot(range(1, EPOCHS+1), hist_sem, marker='s', label='SEM atenção',         color='tomato',    linestyle='--')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Cross-Entropy Loss')\n",
        "plt.title('Curva de Loss: COM vs SEM Atenção')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji-Q8L0oyxQG"
      },
      "source": [
        "## 6. Comparação qualitativa: predições nos mesmos exemplos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMcqaGMmyxQG"
      },
      "outputs": [],
      "source": [
        "print(f\"{'Entrada':<10} {'Esperado':<10} {'COM atenção':<15} {'SEM atenção':<15} {'Correto (COM)':<15} {'Correto (SEM)'}\")\n",
        "print(\"-\" * 85)\n",
        "\n",
        "acertos_com = 0\n",
        "acertos_sem = 0\n",
        "N_TEST = 20\n",
        "\n",
        "for _ in range(N_TEST):\n",
        "    s        = random_seq()\n",
        "    esperado = s[::-1]\n",
        "    pred_com = predict(model_com, s, max_len=len(s))\n",
        "    pred_sem = predict(model_sem, s, max_len=len(s))\n",
        "    ok_com   = '✓' if pred_com == esperado else '✗'\n",
        "    ok_sem   = '✓' if pred_sem == esperado else '✗'\n",
        "    acertos_com += pred_com == esperado\n",
        "    acertos_sem += pred_sem == esperado\n",
        "    print(f\"{s:<10} {esperado:<10} {pred_com:<15} {pred_sem:<15} {ok_com:<15} {ok_sem}\")\n",
        "\n",
        "print(\"-\" * 85)\n",
        "print(f\"Acurácia  →  COM atenção: {acertos_com}/{N_TEST} ({100*acertos_com/N_TEST:.0f}%)   |   SEM atenção: {acertos_sem}/{N_TEST} ({100*acertos_sem/N_TEST:.0f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_7wddfjyxQH"
      },
      "source": [
        "## 7. Conclusões\n",
        "\n",
        "| Aspecto | COM Atenção (Luong) | SEM Atenção |\n",
        "|---|---|---|\n",
        "| **Mecanismo** | O decoder consulta *todos* os estados do encoder a cada passo, ponderando qual posição de entrada é mais relevante | O decoder depende apenas do vetor `h` final do encoder, que deve comprimir toda a sequência |\n",
        "| **Loss esperada** | Menor — o modelo aprende a alinhar posições de saída com as de entrada | Maior — o \"gargalo\" do vetor `h` dificulta aprender o alinhamento |\n",
        "| **Acurácia esperada** | Mais alta | Mais baixa |\n",
        "| **Interpretabilidade** | Os pesos de atenção revelam *qual* parte da entrada o modelo foca em cada passo de saída | Caixa-preta comprimida em `h` |\n",
        "| **Custo computacional** | Levemente maior (produto escalar + softmax por passo) | Menor |\n",
        "\n",
        "### Por que a atenção ajuda nesta tarefa?\n",
        "\n",
        "Inverter uma sequência exige um **alinhamento posicional preciso**: o 1.º token de saída deve corresponder ao último token de entrada, e assim por diante. Com atenção, o decoder aprende exatamente esse mapa — o peso de atenção no passo `t` deve ser alto para a posição `(n - t)` da entrada. Sem atenção, o encoder precisa codificar toda essa informação posicional em um único vetor de tamanho fixo, o que é muito mais difícil."
      ]
    }
  ]
}