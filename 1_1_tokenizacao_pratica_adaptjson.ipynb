{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alxmarqs/LLMtopics/blob/main/1_1_tokenizacao_pratica_adaptjson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFO10S8V18D8"
      },
      "source": [
        "# Aula 2 - Tokenização\n",
        "\n",
        "Corpus utilizado: **100 frases de Marco Aurélio** (*Meditações*).\n",
        "\n",
        "Estrutura desta aula:\n",
        "1. Pré-tokenização\n",
        "2. Treinamento (BPE e WordPiece)\n",
        "3. Pipeline de Encode completo\n",
        "4. ByteLevel vs SentencePiece\n",
        "5. Avaliação comparativa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5oddF7n18D-"
      },
      "source": [
        "## Configuração: Corpus de Marco Aurélio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkahmVcC18D-"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Corpus: 100 frases de Marco Aurélio\n",
        "# ------------------------------------------------------------------------------\n",
        "frases_marco_aurelio = [\n",
        "    \"A felicidade da sua vida depende da qualidade dos seus pensamentos.\",\n",
        "    \"Não perca mais tempo discutindo sobre o que um bom homem deve ser. Seja um.\",\n",
        "    \"Você tem poder sobre sua mente, não sobre eventos externos. Perceba isso e encontrará a força.\",\n",
        "    \"Tudo o que ouvimos é uma opinião, não um fato.\",\n",
        "    \"Tudo o que vemos é uma perspectiva, não a verdade.\",\n",
        "    \"A alma é tingida pela cor dos seus pensamentos.\",\n",
        "    \"A melhor vingança é ser diferente de quem causou o dano.\",\n",
        "    \"Aceite as coisas às quais o destino o ata e ame as pessoas com quem o destino o une.\",\n",
        "    \"Faça cada coisa na vida como se fosse a última.\",\n",
        "    \"A morte sorri para todos nós; tudo o que um homem pode fazer é sorrir de volta.\",\n",
        "    \"O que não é bom para a colmeia também não é bom para a abelha.\",\n",
        "    \"A perda nada mais é do que mudança, e a mudança é o deleite da natureza.\",\n",
        "    \"Se algo é possível para qualquer homem, considere que também é possível para você.\",\n",
        "    \"A vida de um homem é o que seus pensamentos fazem dela.\",\n",
        "    \"Olhe para dentro. Dentro está a fonte do bem, e ela sempre jorrará se você sempre cavar.\",\n",
        "    \"Aquele que vive em harmonia consigo mesmo vive em harmonia com o universo.\",\n",
        "    \"Muitas vezes me perguntei como é que todo homem se ama mais do que a todos os outros homens.\",\n",
        "    \"A arte de viver assemelha-se mais à luta do que à dança.\",\n",
        "    \"O universo é mudança; a vida é opinião.\",\n",
        "    \"Não aja como se fosse viver dez mil anos. A morte paira sobre você.\",\n",
        "    \"Enquanto viver e enquanto puder, seja bom.\",\n",
        "    \"Quão ridículo e quão estranho é surpreender-se com qualquer coisa que acontece na vida.\",\n",
        "    \"O objetivo da vida não é estar do lado da maioria, mas escapar de se encontrar nas fileiras dos loucos.\",\n",
        "    \"O homem deve ter medo não da morte, mas de nunca começar a viver.\",\n",
        "    \"A pobreza é a mãe do crime.\",\n",
        "    \"Lembre-se de que tudo o que acontece, acontece como deveria.\",\n",
        "    \"Nada acontece a homem algum que ele não seja formado pela natureza para suportar.\",\n",
        "    \"A rejeição é a sua própria defesa.\",\n",
        "    \"A primeira regra é manter o espírito tranquilo.\",\n",
        "    \"A segunda regra é olhar as coisas de frente e saber o que elas são.\",\n",
        "    \"Não se deixe levar pelo futuro.\",\n",
        "    \"Sempre que estiver prestes a apontar um defeito em outra pessoa, questione seus próprios defeitos.\",\n",
        "    \"A vida não é boa nem má, mas apenas o lugar para o bem e o mal.\",\n",
        "    \"Se não for certo, não faça; se não for verdade, não diga.\",\n",
        "    \"Concentre-se a cada minuto como um romano e um homem.\",\n",
        "    \"Faça o que tem à sua frente com seriedade precisa e genuína.\",\n",
        "    \"Liberte-se de todas as outras distrações.\",\n",
        "    \"A brevidade da vida é a única certeza.\",\n",
        "    \"Em breve você terá esquecido tudo; em breve todos terão esquecido você.\",\n",
        "    \"O tempo é um rio, um fluxo violento de eventos.\",\n",
        "    \"Assim que algo é avistado, é arrastado e outra coisa toma o seu lugar.\",\n",
        "    \"Pense na beleza da vida. Observe as estrelas e veja-se correndo com elas.\",\n",
        "    \"Tudo o que é belo em si mesmo é completo em si mesmo e não precisa de elogios.\",\n",
        "    \"O elogio não torna nada melhor ou pior.\",\n",
        "    \"Se você está angustiado por qualquer coisa externa, a dor não se deve à coisa em si.\",\n",
        "    \"Você tem o poder de revogar sua estimativa a qualquer momento.\",\n",
        "    \"Seja como o promontório contra o qual as ondas quebram continuamente.\",\n",
        "    \"Permaneça firme e dome a fúria da água ao seu redor.\",\n",
        "    \"Não fui ferido pelo presente nem tenho medo do futuro.\",\n",
        "    \"Adapte-se ao ambiente em que sua sorte o colocou.\",\n",
        "    \"Ame verdadeiramente os companheiros com quem o destino o ordenou caminhar.\",\n",
        "    \"Olhe para o passado, para os impérios em ascensão e queda.\",\n",
        "    \"Não há nada de novo sob o sol.\",\n",
        "    \"Nenhuma pessoa tem o poder de ter tudo o que deseja.\",\n",
        "    \"Está em seu poder não querer o que você não tem.\",\n",
        "    \"A tranquilidade não é nada além da boa ordenação da mente.\",\n",
        "    \"O mundo é uma cidade.\",\n",
        "    \"Para viver feliz, basta muito pouco.\",\n",
        "    \"Tudo está interligado como uma teia sagrada.\",\n",
        "    \"Nenhuma coisa é estranha à outra.\",\n",
        "    \"Os homens existem para o bem uns dos outros.\",\n",
        "    \"Ensine-os ou suporte-os.\",\n",
        "    \"A injustiça é uma impiedade.\",\n",
        "    \"Aquele que comete injustiça, comete injustiça contra si mesmo.\",\n",
        "    \"Muitas vezes peca quem deixa de fazer algo, não apenas quem faz algo.\",\n",
        "    \"Comece cada dia dizendo a si mesmo: Hoje encontrarei interferência, ingratidão, insolência.\",\n",
        "    \"Mas eu vi a beleza do bem e a feiura do mal.\",\n",
        "    \"Reconheço que o malfeitor tem uma natureza semelhante à minha.\",\n",
        "    \"Ninguém pode me prejudicar, pois ninguém pode me envolver em baixeza.\",\n",
        "    \"Nascemos para trabalhar juntos como pés, mãos e olhos.\",\n",
        "    \"Obstruir uns aos outros é antinatural.\",\n",
        "    \"Sentir raiva e virar as costas para alguém é obstrução.\",\n",
        "    \"O que morre não cai fora do mundo.\",\n",
        "    \"Se fica aqui, muda aqui e se dissolve em suas partes apropriadas.\",\n",
        "    \"A mudança não é um mal, assim como a persistência no novo estado não é um bem.\",\n",
        "    \"Lembre-se de quanto tempo você adiou essas coisas.\",\n",
        "    \"Quantas vezes você recebeu uma oportunidade dos deuses e não a usou.\",\n",
        "    \"Você deve finalmente perceber de que tipo de mundo você é uma parte.\",\n",
        "    \"Existe um limite para o seu tempo.\",\n",
        "    \"O tempo nunca retornará.\",\n",
        "    \"Descarte sua sede de livros, para que não morra murmurando.\",\n",
        "    \"Morra verdadeiramente resignado e grato de coração aos deuses.\",\n",
        "    \"Considere continuamente como todas as coisas que existem agora existiram no passado.\",\n",
        "    \"Dramas e cenas inteiras são sempre os mesmos, apenas os atores mudam.\",\n",
        "    \"Não despreze a morte, mas aceite-a de bom grado.\",\n",
        "    \"A morte é uma das coisas que a natureza deseja.\",\n",
        "    \"Busque a sabedoria, não o conhecimento.\",\n",
        "    \"O conhecimento é o acúmulo de fatos, a sabedoria é a simplificação.\",\n",
        "    \"A melhor maneira de se vingar de um inimigo é não se assemelhar a ele.\",\n",
        "    \"Onde um homem pode viver, ele também pode viver bem.\",\n",
        "    \"É possível viver bem em um palácio? Sim.\",\n",
        "    \"Então é possível viver bem em qualquer lugar.\",\n",
        "    \"Olhe sob a superfície; não deixe que a qualidade ou o valor de nada lhe escape.\",\n",
        "    \"A felicidade é um bom fluxo da vida.\",\n",
        "    \"Temos dois ouvidos e uma boca, para ouvirmos mais do que falamos.\",\n",
        "    \"O homem conquista o mundo conquistando a si mesmo.\",\n",
        "    \"Nenhuma perda deve ser mais dolorosa para nós do que a perda do tempo.\",\n",
        "    \"Siga para onde a razão levar.\",\n",
        "    \"O destino é a cadeia infinita de causalidade, por onde as coisas existem.\",\n",
        "]\n",
        "\n",
        "# Salva como JSONL (formato recomendado pela biblioteca tokenizers)\n",
        "ARQUIVO_CORPUS = \"corpus_marco_aurelio.jsonl\"\n",
        "with open(ARQUIVO_CORPUS, \"w\", encoding=\"utf-8\") as f:\n",
        "    for frase in frases_marco_aurelio:\n",
        "        f.write(json.dumps({\"text\": frase}, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "def carregar_corpus_jsonl(caminho):\n",
        "    with open(caminho, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            yield json.loads(line)[\"text\"]\n",
        "\n",
        "print(f\"Corpus criado: {len(frases_marco_aurelio)} frases.\")\n",
        "print(f\"Exemplo: {frases_marco_aurelio[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iuunjgg718D_"
      },
      "source": [
        "---\n",
        "## Parte 1 - Pré-tokenização\n",
        "\n",
        "Comparamos quatro estratégias de pré-tokenização sobre uma frase do corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XL5uXaH18D_"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer, models, pre_tokenizers\n",
        "\n",
        "frase = \"A felicidade da sua vida depende da qualidade dos seus pensamentos.\"\n",
        "\n",
        "# --- Whitespace ---\n",
        "tok_ws = Tokenizer(models.BPE())\n",
        "tok_ws.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "print(\"Whitespace:\")\n",
        "print(tok_ws.pre_tokenizer.pre_tokenize_str(frase))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km05tLvb18EA"
      },
      "source": [
        "### Punctuation + Whitespace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FX5Dxys718EA"
      },
      "outputs": [],
      "source": [
        "tok_punc = Tokenizer(models.BPE())\n",
        "tok_punc.pre_tokenizer = pre_tokenizers.Sequence([\n",
        "    pre_tokenizers.Whitespace(),\n",
        "    pre_tokenizers.Punctuation()\n",
        "])\n",
        "print(\"Punctuation + Whitespace:\")\n",
        "print(tok_punc.pre_tokenizer.pre_tokenize_str(frase))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdfXYFBy18EA"
      },
      "source": [
        "### ByteLevel (estilo GPT-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZ9Nbr_b18EA"
      },
      "outputs": [],
      "source": [
        "tok_byte = Tokenizer(models.BPE())\n",
        "tok_byte.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
        "print(\"ByteLevel (GPT-2 style):\")\n",
        "print(tok_byte.pre_tokenizer.pre_tokenize_str(frase))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY3D_Rn_18EA"
      },
      "source": [
        "### Metaspace (SentencePiece style)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyIjRFSx18EA"
      },
      "outputs": [],
      "source": [
        "tok_meta = Tokenizer(models.BPE())\n",
        "tok_meta.pre_tokenizer = pre_tokenizers.Metaspace()\n",
        "print(\"Metaspace (SentencePiece style):\")\n",
        "print(tok_meta.pre_tokenizer.pre_tokenize_str(frase))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr0qAxhA18EB"
      },
      "source": [
        "---\n",
        "## Parte 2 - Treinamento\n",
        "\n",
        "Treinamos dois modelos sobre o corpus de Marco Aurélio:\n",
        "- **BPE** (Byte Pair Encoding)\n",
        "- **WordPiece** (estilo BERT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnScjGZT18EB"
      },
      "outputs": [],
      "source": [
        "from tokenizers import trainers\n",
        "\n",
        "# Visão geral dos algoritmos\n",
        "print(\"=== BPE ===\")\n",
        "print(\"1. Começa com todos os caracteres do corpus como tokens.\")\n",
        "print(\"2. Encontra e une o par de tokens mais frequente em um novo token.\")\n",
        "print(\"3. Repete até atingir o tamanho de vocabulário desejado.\\n\")\n",
        "\n",
        "print(\"=== WordPiece ===\")\n",
        "print(\"1. Similar ao BPE, mas usa uma função de pontuação: freq(AB) / (freq(A) * freq(B)).\")\n",
        "print(\"2. Favorece pares que aumentam a verossimilhança do modelo.\")\n",
        "print(\"3. Sufixos são marcados com '##' para indicar continuidade.\\n\")\n",
        "\n",
        "VOCAB_SIZE    = 1000\n",
        "SPECIAL_TOKENS = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        "\n",
        "# --- Treino BPE ---\n",
        "print(\"Treinando BPE...\")\n",
        "tokenizer_bpe = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer_bpe.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "trainer_bpe = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=SPECIAL_TOKENS)\n",
        "tokenizer_bpe.train_from_iterator(carregar_corpus_jsonl(ARQUIVO_CORPUS), trainer=trainer_bpe)\n",
        "tokenizer_bpe.save(\"tokenizer_bpe.json\")\n",
        "\n",
        "# --- Treino WordPiece ---\n",
        "print(\"Treinando WordPiece...\")\n",
        "tokenizer_wp = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
        "tokenizer_wp.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "tokenizer_wp.decoder = decoders.WordPiece()\n",
        "trainer_wp = trainers.WordPieceTrainer(vocab_size=VOCAB_SIZE, special_tokens=SPECIAL_TOKENS)\n",
        "tokenizer_wp.train_from_iterator(carregar_corpus_jsonl(ARQUIVO_CORPUS), trainer=trainer_wp)\n",
        "tokenizer_wp.save(\"tokenizer_wp.json\")\n",
        "\n",
        "# Exibe parte do vocabulário BPE\n",
        "vocab = tokenizer_bpe.get_vocab()\n",
        "print(f\"\\nTamanho do vocabulário BPE: {len(vocab)}\")\n",
        "sorted_vocab = sorted(vocab.items(), key=lambda kv: kv[1])[:20]\n",
        "for token, idx in sorted_vocab:\n",
        "    print(f\"{idx:>3} → {repr(token)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOYvoS7H18EB"
      },
      "source": [
        "### Teste rápido dos modelos treinados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qbhwDe818EB"
      },
      "outputs": [],
      "source": [
        "tok_bpe = Tokenizer.from_file(\"tokenizer_bpe.json\")\n",
        "tok_wp  = Tokenizer.from_file(\"tokenizer_wp.json\")\n",
        "\n",
        "textos_teste = [\n",
        "    \"A vida de um homem é o que seus pensamentos fazem dela.\",\n",
        "    \"Anticonstitucionalissimamente\",\n",
        "    \"Data science e machine learning transformam o agro.\"\n",
        "]\n",
        "\n",
        "print(\"\\nTokenização de exemplos (BPE):\")\n",
        "for t in textos_teste:\n",
        "    out = tok_bpe.encode(t)\n",
        "    print(f\"  Texto  : {t}\")\n",
        "    print(f\"  Tokens : {out.tokens}\")\n",
        "    print(f\"  IDs    : {out.ids}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrPUjEMA18EB"
      },
      "source": [
        "---\n",
        "## Parte 3 - Encode\n",
        "\n",
        "Pipeline completo: **normalização → pré-tokenização → modelo → pós-processamento**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1N_rrIe18EB"
      },
      "outputs": [],
      "source": [
        "from tokenizers import normalizers, processors\n",
        "from tokenizers.normalizers import NFD, StripAccents, Lowercase\n",
        "from tokenizers.pre_tokenizers import Whitespace, Digits, Sequence\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "print(\"### Pipeline de tokenização ###\")\n",
        "print(\" 1. Normalization\")\n",
        "print(\" 2. Pre-tokenization\")\n",
        "print(\" 3. Model\")\n",
        "print(\" 4. Post-processing\\n\")\n",
        "\n",
        "# Carrega o tokenizador BPE treinado no corpus de Marco Aurélio\n",
        "tokenizer = Tokenizer.from_file(\"tokenizer_bpe.json\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1. Normalization\n",
        "# ------------------------------------------------------------------\n",
        "print(\"# Normalization\")\n",
        "normalizer = normalizers.Sequence([\n",
        "    NFD(),          # decompõe acentos em caracteres separados\n",
        "    Lowercase(),    # tudo minúsculo\n",
        "    StripAccents()  # remove acentos\n",
        "])\n",
        "texto_teste = \"A Tranqüilidade da Mente é Essencial.\"\n",
        "print(\"Antes :\", texto_teste)\n",
        "print(\"Depois:\", normalizer.normalize_str(texto_teste), \"\\n\")\n",
        "tokenizer.normalizer = normalizer\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2. Pre-tokenization\n",
        "# ------------------------------------------------------------------\n",
        "print(\"# Pre-tokenization\")\n",
        "pre_tok = Sequence([\n",
        "    Whitespace(),\n",
        "    Digits(individual_digits=True)\n",
        "])\n",
        "texto_numeros = \"Marco Aurélio nasceu em 121 d.C. e morreu em 180 d.C.\"\n",
        "print(\"Pré-tokenização:\", pre_tok.pre_tokenize_str(texto_numeros), \"\\n\")\n",
        "tokenizer.pre_tokenizer = pre_tok\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3. Model: BPE (já carregado)\n",
        "# ------------------------------------------------------------------\n",
        "print(\"# Model: BPE (treinado no corpus de Marco Aurélio)\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 4. Post-processing\n",
        "# ------------------------------------------------------------------\n",
        "print(\"# Post-processing (TemplateProcessing — estilo BERT)\")\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[(\"[CLS]\", 2), (\"[SEP]\", 3)],\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Aplicando o pipeline completo\n",
        "# ------------------------------------------------------------------\n",
        "frases_encode = [\n",
        "    \"A vida de um homem é o que seus pensamentos fazem dela.\",\n",
        "    \"O universo é mudança; a vida é opinião.\",\n",
        "]\n",
        "for f in frases_encode:\n",
        "    encoded = tokenizer.encode(f)\n",
        "    print(f\"\\nFrase  : {f}\")\n",
        "    print(f\"Tokens : {encoded.tokens}\")\n",
        "    print(f\"IDs    : {encoded.ids}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqvB59IS18EB"
      },
      "source": [
        "---\n",
        "## Parte 4 - ByteLevel vs SentencePiece\n",
        "\n",
        "Comparamos os tokenizadores pré-treinados GPT-2 (ByteLevel) e mT5 (SentencePiece/Unigram) sobre frases do corpus de Marco Aurélio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqaSE8k718EC"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import unicodedata\n",
        "\n",
        "BYTELEVEL_MODEL = \"openai-community/gpt2\"\n",
        "SENTPIECE_MODEL = \"google/mt5-small\"\n",
        "\n",
        "tok_byte_pt = AutoTokenizer.from_pretrained(BYTELEVEL_MODEL)\n",
        "tok_spm     = AutoTokenizer.from_pretrained(SENTPIECE_MODEL)\n",
        "\n",
        "if tok_byte_pt.pad_token is None:\n",
        "    tok_byte_pt.pad_token = tok_byte_pt.eos_token\n",
        "\n",
        "# Frase com caracteres acentuados e pontuação — típica do corpus\n",
        "texto = \"A tranqüilidade não é nada além da boa ordenação da mente.\"\n",
        "print(f\"Texto: {texto}\\n\")\n",
        "\n",
        "def encode_details(tokenizer, name):\n",
        "    enc    = tokenizer(texto, add_special_tokens=True, return_offsets_mapping=True)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"])\n",
        "    ids    = enc[\"input_ids\"]\n",
        "    offsets = enc[\"offset_mapping\"]\n",
        "    print(f\"=== {name} ===\")\n",
        "    print(\"Tokens     :\", tokens)\n",
        "    print(\"IDs        :\", ids)\n",
        "    print(\"Qtd tokens :\", len(tokens))\n",
        "    print(\"Decoded    :\", tokenizer.decode(ids))\n",
        "    print(\"Offsets    :\", offsets)\n",
        "    print()\n",
        "\n",
        "encode_details(tok_byte_pt, \"ByteLevel (GPT-2)\")\n",
        "encode_details(tok_spm,     \"SentencePiece (mT5)\")\n",
        "\n",
        "# Inspecionando os caracteres Unicode da frase\n",
        "print(\"Caracteres Unicode:\")\n",
        "for ch in texto:\n",
        "    name = unicodedata.name(ch, \"UNKNOWN\")\n",
        "    print(f\"  {repr(ch)} → {name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynTHQB2y18EC"
      },
      "source": [
        "---\n",
        "## Parte 5 - Avaliação\n",
        "\n",
        "Métricas de eficiência dos dois modelos treinados (BPE vs WordPiece) sobre uma amostra do corpus de Marco Aurélio, incluindo frases do domínio e frases técnicas fora do domínio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOt8t_OY18EC"
      },
      "outputs": [],
      "source": [
        "tok_bpe = Tokenizer.from_file(\"tokenizer_bpe.json\")\n",
        "tok_wp  = Tokenizer.from_file(\"tokenizer_wp.json\")\n",
        "\n",
        "# Corpus de teste: frases dentro e fora do domínio\n",
        "frases_avaliacao = [\n",
        "    # Dentro do domínio (espera-se alta eficiência)\n",
        "    \"A felicidade da sua vida depende da qualidade dos seus pensamentos.\",\n",
        "    \"A vida de um homem é o que seus pensamentos fazem dela.\",\n",
        "    \"A tranquilidade não é nada além da boa ordenação da mente.\",\n",
        "    \"O homem conquista o mundo conquistando a si mesmo.\",\n",
        "    \"Siga para onde a razão levar.\",\n",
        "    \"Olhe para dentro. Dentro está a fonte do bem.\",\n",
        "    # Fora do domínio (espera-se quebra de tokens)\n",
        "    \"SeedMetrics utiliza inteligência artificial e visão computacional.\",\n",
        "    \"Anticonstitucionalissimamente é uma palavra longa.\",\n",
        "    \"Data science e machine learning transformam o agro.\",\n",
        "    \"O sistema de análise de sementes falhou no upload.\",\n",
        "]\n",
        "\n",
        "def avaliar_modelo(tokenizer, nome, textos):\n",
        "    dados = []\n",
        "    for t in textos:\n",
        "        enc = tokenizer.encode(t)\n",
        "        dados.append({\n",
        "            \"Modelo\":      nome,\n",
        "            \"Texto\":       t[:35] + \"...\" if len(t) > 35 else t,\n",
        "            \"Chars\":       len(t),\n",
        "            \"Palavras\":    len(t.split()),\n",
        "            \"Tokens\":      len(enc.tokens),\n",
        "            \"<UNK>\": enc.tokens.count(\"[UNK]\"),\n",
        "            \"Reversível\":  tokenizer.decode(enc.ids) == t,\n",
        "        })\n",
        "    return pd.DataFrame(dados)\n",
        "\n",
        "df_bpe = avaliar_modelo(tok_bpe, \"BPE\",       frases_avaliacao)\n",
        "df_wp  = avaliar_modelo(tok_wp,  \"WordPiece\", frases_avaliacao)\n",
        "\n",
        "df_final = pd.concat([df_bpe, df_wp]).sort_values([\"Texto\", \"Modelo\"])\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "print(\"=== TABELA COMPARATIVA ===\")\n",
        "print(df_final.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYMGEL4618EC"
      },
      "outputs": [],
      "source": [
        "# --- Métricas agregadas ---\n",
        "for df, nome in [(df_bpe, \"BPE\"), (df_wp, \"WordPiece\")]:\n",
        "    tpc        = (df[\"Tokens\"] / df[\"Chars\"]).mean()\n",
        "    tpw        = (df[\"Tokens\"] / df[\"Palavras\"]).mean()\n",
        "    unk_rate   = (df[\"<UNK>\"].sum() / df[\"Tokens\"].sum()) * 100\n",
        "    rev_acc    = df[\"Reversível\"].mean() * 100\n",
        "    print(f\"=== {nome} ===\")\n",
        "    print(f\"  Tokens por caractere (TPC) : {tpc:.3f}\")\n",
        "    print(f\"  Tokens por palavra   (TPW) : {tpw:.3f}\")\n",
        "    print(f\"  Percentual de [UNK]        : {unk_rate:.2f}%\")\n",
        "    print(f\"  Reversibilidade            : {rev_acc:.1f}%\")\n",
        "    print(f\"  Tam. médio de sequência    : {df['Tokens'].mean():.1f} tokens/frase\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVNXZ0c918EC"
      },
      "outputs": [],
      "source": [
        "# --- Visualização: Quantidade de tokens por frase ---\n",
        "labels = [t[:25] + \"...\" if len(t) > 25 else t for t in frases_avaliacao]\n",
        "x = np.arange(len(labels))\n",
        "w = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(13, 5))\n",
        "ax.bar(x - w/2, df_bpe[\"Tokens\"].values, width=w, label=\"BPE\",       color=\"steelblue\")\n",
        "ax.bar(x + w/2, df_wp[\"Tokens\"].values,  width=w, label=\"WordPiece\", color=\"tomato\", alpha=0.85)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels, rotation=40, ha=\"right\", fontsize=8)\n",
        "ax.set_ylabel(\"Quantidade de tokens\")\n",
        "ax.set_title(\"BPE vs WordPiece — Tokens por frase (corpus Marco Aurélio)\")\n",
        "ax.legend()\n",
        "ax.axvline(x=5.5, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"Domínio | Fora do domínio\")\n",
        "ax.text(2.5, ax.get_ylim()[1]*0.92, \"No domínio\",    ha=\"center\", color=\"gray\", fontsize=9)\n",
        "ax.text(8,   ax.get_ylim()[1]*0.92, \"Fora do domínio\", ha=\"center\", color=\"gray\", fontsize=9)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSoW1D--18EC"
      },
      "outputs": [],
      "source": [
        "# --- Raio-X do vocabulário: BPE vs WordPiece ---\n",
        "vocab_bpe = set(tok_bpe.get_vocab().keys())\n",
        "vocab_wp  = set(tok_wp.get_vocab().keys())\n",
        "\n",
        "intersecao      = vocab_bpe & vocab_wp\n",
        "exclusivos_bpe  = vocab_bpe - vocab_wp\n",
        "exclusivos_wp   = vocab_wp  - vocab_bpe\n",
        "\n",
        "print(\"=== Raio-X do Vocabulário ===\")\n",
        "print(f\"Total BPE      : {len(vocab_bpe)}\")\n",
        "print(f\"Total WP       : {len(vocab_wp)}\")\n",
        "print(f\"Em comum       : {len(intersecao)}\")\n",
        "print(f\"Exclusivo BPE  : {len(exclusivos_bpe)} tokens\")\n",
        "print(f\"Exclusivo WP   : {len(exclusivos_wp)} tokens\")\n",
        "\n",
        "print(\"\\nExemplos exclusivos do BPE (sufixos fundidos):\")\n",
        "print(list(exclusivos_bpe)[:10])\n",
        "\n",
        "print(\"\\nExemplos exclusivos do WordPiece (prefixos ##):\")\n",
        "print(list(exclusivos_wp)[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tJztVzF18ED"
      },
      "source": [
        "---\n",
        "## Conclusões\n",
        "\n",
        "| Aspecto | BPE | WordPiece |\n",
        "|---|---|---|\n",
        "| **Critério de fusão** | Frequência bruta do par | Maximiza verossimilhança: `freq(AB)/(freq(A)×freq(B))` |\n",
        "| **Marcação de subpalavras** | Sem marcação especial | Sufixos marcados com `##` |\n",
        "| **Decoder** | Concatenação direta | Remove os `##` antes de concatenar |\n",
        "| **Vocabulário** | Tende a fundir sufixos frequentes | Tende a preservar prefixos comuns |\n",
        "| **Eficiência no domínio** | Alta (corpus português filosófico) | Alta (similar ao BPE neste corpus) |\n",
        "| **Fora do domínio** | Quebra em caracteres individuais | Idem — ambos não conhecem o vocabulário técnico |\n",
        "\n",
        "**Observação sobre o corpus:** Por ser um corpus pequeno (100 frases) em português, ambos os modelos apresentam comportamento similar dentro do domínio. Palavras técnicas em inglês ou compostas (\"Anticonstitucionalissimamente\", \"SeedMetrics\") geram muitos tokens `[UNK]` ou subpalavras de 1 caractere, evidenciando a importância do corpus de treinamento na qualidade da tokenização."
      ]
    }
  ]
}